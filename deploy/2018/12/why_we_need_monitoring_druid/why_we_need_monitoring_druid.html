<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Why We Need Monitoring Druid - Jason Joo</title>
        <script>if (top !== self) top.location = self.location;</script>
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=0" />
        <link rel="stylesheet" href="/static/style.css?v=2c4d4" />
        <link rel="stylesheet" href="/static/pygments.css?v=c6adc" />
        
        <link rel="alternate" type="application/rss+xml" href="/feed.xml" title="Jason Joo" />
        <!--[if lt IE 9]>
        <script type="text/javascript" src="/static/html5shiv.js?v=9a10c"></script>
        <![endif]-->
        
        
<link rel="canonical" href="/why_we_need_monitoring_druid.html" />
<meta name="description" content="Recently there is a very strange problem happening every night on about 20:00:05. The <code>SELECTED</code> provider (DUBBO) will reach the capacity of DUBBO thread pool which is limit to <strong>2000</strong> threads and in <strong>fixed</strong> pool model. Most of them blocked in the position of fetching connections through druid:

<code>java
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@63c4ac4b
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at com.alibaba.druid.pool.DruidDataSource.pollLast(DruidDataSource.java:1479)
        at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1079)
        at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:946)
        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:4544)
        at com.alibaba.druid.filter.stat.StatFilter.dataSource_getConnection(StatFilter.java:661)
        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:4540)
</code>

First we can find that it seems like many threads stuck inside pollLast(). Precisely on <code>notEmpty.awaitNanos()</code>. Maybe it is caused by some forms of dead lock or racing problem. Which is even more strange why we use <code>SELECTED</code> before there is only <code>ONE</code> node suffered this issue while we have 6 nodes all together.

So There maybe some rarely executed invocation or scheduled task causing this problem.

Currently we use the version <code>1.0.10</code> while the newest release is <code>1.1.12</code>. So we can try to upgrade it.

Besides we can introduce more metrics of the <code>Druid Pool</code> to <code>Open Falcon</code> and see whether there are some other reasons.

We will try:

First we found the curve of <code>NotEmptyWaitCount</code> telling that there maybe some problems in the pool at some traffic fluctuation points(Shown below).  

<img src="NotEmptyWaitCount.png" alt="NotEmptyWaitCount">

We can see when sudden traffic incoming the wait count increased.  

And we compared another indicator <code>PhysicalConnect</code>:  

<img src="PhysicalConnect.png" alt="PhysicalConnect">  

We can also be sure when wait count increased some physical connecting actions were really happened.

After some iteration of pool parameters including increasing <code>minIdle</code> and <code>initialSize</code> we checked the indicator <code>PoolingCount</code> which can indicate the connection count in idle.  

<img src="PoolingCount.png" alt="PoolingCount">  

We can see the pool can&#39;t keep the count of idle connections as we expected(minIdle = initialSize = 20). And it keeps changing after some minutes. So if recycleThread didn&#39;t recycle them there must be some other reasons causing them destroyed. And we checked that we enabled <code>testWhileIdle</code> while disabling <code>testOnBorrow</code> and <code>testOnReturn</code>. We thought that idle connections will be checked periodically by setting <code>testWhileIdle</code> but things didn&#39;t indicate the same result.  

After reading the source code for a while we found there was really a misunderstanding on <code>testWhileIdle</code>.

<code>initialSize</code> means when pool starts how many connections will be created.<br>
<code>minIdle</code> means when evictThread runs how many connections will prevent to be destroyed.  

So generally <code>initialSize</code> may be greater or equals to <code>minIdle</code>.

<code>minEvictableIdleTimeMillis</code> means how long a connection turns to be useless when in idle. But when it&#39;s in <code>minIdle</code> range <strong>it will ignore this</strong>.

<code>timeBetweenEvictionRunsMillis</code> means the time between recycle thread&#39;s loop.

<code>testWhileIdle</code> doesn&#39;t mean test occurred while a connection is in idle but when a connection is borrowed and has been in idle more then <code>timeBetweenEvictionRunsMillis</code> the connection test will happen.  

That is <strong>test when borrowed a long-idle connection</strong>. The test will only happen in getConnection as <code>testOnBorrow</code> means which is more efficient.  

So if you have some timeout settings on server side you may want to use <code>keepAlive</code>.

<code>keepAlive</code> means when a connection is in idle a connection test will happen on it every <code>timeBetweenEvictionRunsMillis</code>. It enhances the recycleThread.  

We can be sure of all idle connections&#39;s status using it.

<img src="PoolingCountAfter.png" alt="poolCount&#39;s curve after adding keepAlive">

Connections in idle pool(Which is calculated as <code>poolCount</code>) is not reliable when you turn off <code>testOnBorrow</code>. And if you have some big fluctuations of load, for example, you will also take time to reconnect with turning on <code>testOnBorrow</code>. That  will lead to poor performance when fluctuations occur.  

So we can turn on the <code>keepAlive</code> option to really keep alive the connections in idle (<code>minIdle</code>) to get prepared to the fluctuations.

<code>xml
&lt;property name=&quot;initialSize&quot; value=&quot;40&quot; /&gt;
&lt;property name=&quot;minIdle&quot; value=&quot;40&quot; /&gt;
&lt;property name=&quot;maxActive&quot; value=&quot;1000&quot; /&gt;
&lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt;
&lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;180000&quot; /&gt;
&lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;600000&quot; /&gt;
&lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#39;x&#39;&quot; /&gt;
&lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;
&lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;
&lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;
&lt;property name=&quot;keepAlive&quot; value=&quot;true&quot; /&gt;
</code>

By the way the problem of <code>NotEmptyWaitCount</code> is solved at the same time.
<img src="NotEmptyWaitCountAfter.png" alt="NotEmptyWaitCountAfter">" />

    </head>
    <body>
        <header id="header">
            <h1 id="brand"><a href="/">Jason Joo</a></h1>
            <nav id="nav" role="navigation">
                
                <a href="/"></a>
                <a href="/about.html"></a>
            </nav>
        </header>

        <div id="main">
            <div class="container">
<div class="hentry">
    <h1 class="entry-title">Why We Need Monitoring Druid</h1>
    <div class="entry-description">Recently there is a very strange problem happening every night on about 20:00:05. The <code>SELECTED</code> provider (DUBBO) will reach the capacity of DUBBO thread pool which is limit to <strong>2000</strong> threads and in <strong>fixed</strong> pool model. Most of them blocked in the position of fetching connections through druid:

<code>java
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@63c4ac4b
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
        at com.alibaba.druid.pool.DruidDataSource.pollLast(DruidDataSource.java:1479)
        at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1079)
        at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:946)
        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:4544)
        at com.alibaba.druid.filter.stat.StatFilter.dataSource_getConnection(StatFilter.java:661)
        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:4540)
</code>

First we can find that it seems like many threads stuck inside pollLast(). Precisely on <code>notEmpty.awaitNanos()</code>. Maybe it is caused by some forms of dead lock or racing problem. Which is even more strange why we use <code>SELECTED</code> before there is only <code>ONE</code> node suffered this issue while we have 6 nodes all together.

So There maybe some rarely executed invocation or scheduled task causing this problem.

Currently we use the version <code>1.0.10</code> while the newest release is <code>1.1.12</code>. So we can try to upgrade it.

Besides we can introduce more metrics of the <code>Druid Pool</code> to <code>Open Falcon</code> and see whether there are some other reasons.

We will try:

First we found the curve of <code>NotEmptyWaitCount</code> telling that there maybe some problems in the pool at some traffic fluctuation points(Shown below).  

<img src="NotEmptyWaitCount.png" alt="NotEmptyWaitCount">

We can see when sudden traffic incoming the wait count increased.  

And we compared another indicator <code>PhysicalConnect</code>:  

<img src="PhysicalConnect.png" alt="PhysicalConnect">  

We can also be sure when wait count increased some physical connecting actions were really happened.

After some iteration of pool parameters including increasing <code>minIdle</code> and <code>initialSize</code> we checked the indicator <code>PoolingCount</code> which can indicate the connection count in idle.  

<img src="PoolingCount.png" alt="PoolingCount">  

We can see the pool can&#39;t keep the count of idle connections as we expected(minIdle = initialSize = 20). And it keeps changing after some minutes. So if recycleThread didn&#39;t recycle them there must be some other reasons causing them destroyed. And we checked that we enabled <code>testWhileIdle</code> while disabling <code>testOnBorrow</code> and <code>testOnReturn</code>. We thought that idle connections will be checked periodically by setting <code>testWhileIdle</code> but things didn&#39;t indicate the same result.  

After reading the source code for a while we found there was really a misunderstanding on <code>testWhileIdle</code>.

<code>initialSize</code> means when pool starts how many connections will be created.<br>
<code>minIdle</code> means when evictThread runs how many connections will prevent to be destroyed.  

So generally <code>initialSize</code> may be greater or equals to <code>minIdle</code>.

<code>minEvictableIdleTimeMillis</code> means how long a connection turns to be useless when in idle. But when it&#39;s in <code>minIdle</code> range <strong>it will ignore this</strong>.

<code>timeBetweenEvictionRunsMillis</code> means the time between recycle thread&#39;s loop.

<code>testWhileIdle</code> doesn&#39;t mean test occurred while a connection is in idle but when a connection is borrowed and has been in idle more then <code>timeBetweenEvictionRunsMillis</code> the connection test will happen.  

That is <strong>test when borrowed a long-idle connection</strong>. The test will only happen in getConnection as <code>testOnBorrow</code> means which is more efficient.  

So if you have some timeout settings on server side you may want to use <code>keepAlive</code>.

<code>keepAlive</code> means when a connection is in idle a connection test will happen on it every <code>timeBetweenEvictionRunsMillis</code>. It enhances the recycleThread.  

We can be sure of all idle connections&#39;s status using it.

<img src="PoolingCountAfter.png" alt="poolCount&#39;s curve after adding keepAlive">

Connections in idle pool(Which is calculated as <code>poolCount</code>) is not reliable when you turn off <code>testOnBorrow</code>. And if you have some big fluctuations of load, for example, you will also take time to reconnect with turning on <code>testOnBorrow</code>. That  will lead to poor performance when fluctuations occur.  

So we can turn on the <code>keepAlive</code> option to really keep alive the connections in idle (<code>minIdle</code>) to get prepared to the fluctuations.

<code>xml
&lt;property name=&quot;initialSize&quot; value=&quot;40&quot; /&gt;
&lt;property name=&quot;minIdle&quot; value=&quot;40&quot; /&gt;
&lt;property name=&quot;maxActive&quot; value=&quot;1000&quot; /&gt;
&lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt;
&lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;180000&quot; /&gt;
&lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;600000&quot; /&gt;
&lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#39;x&#39;&quot; /&gt;
&lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;
&lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;
&lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;
&lt;property name=&quot;keepAlive&quot; value=&quot;true&quot; /&gt;
</code>

By the way the problem of <code>NotEmptyWaitCount</code> is solved at the same time.
<img src="NotEmptyWaitCountAfter.png" alt="NotEmptyWaitCountAfter"></div>
    <div class="entry-content">
        
    </div>
</div>
</div>
        </div>

        <footer id="footer">
            <hr class="end" />
            
            <p class="copyright">
            <span class="software">
                Powered by <a href="http://lab.lepture.com/liquidluck/">Felix Felicis</a> 3.8.1,
            </span>
            <span class="theme">
                Theme <a href="https://github.com/lepture/liquidluck-theme-moment" rel="nofollow">moment</a> 1.0 by <a href="http://lepture.com">Hsiaoming Yang</a>
            </span>
            </p>
        </footer>
        <script type="text/javascript" src="/static/mobile.js?v=3b6df"></script>
        
    </body>
</html>